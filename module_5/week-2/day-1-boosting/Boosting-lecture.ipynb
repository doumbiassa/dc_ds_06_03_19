{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Boosting Algorithms\n",
    "\n",
    "## Goals\n",
    "\n",
    "After this lesson you should be able to:\n",
    "\n",
    "- Explain how boosting algorithms work and identify their strengths and weaknesses\n",
    "- Explain the differences between boosting and bagging methods\n",
    "- Understand the AdaBoost algorithm\n",
    "- Understand the difference between AdaBoost and Gradient Boosting\n",
    "- Implement boosting models to solve classification and regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Review\n",
    "\n",
    "- Bootstrap aggregation - resample entire dataset multiple times with replacement. Average model performance on bootstrapped data sets\n",
    "- Uses strong learners (Decision tree, RF, etc. and gives equal votes to each estimator)\n",
    "- Models run in parallel - can be better for scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping - resample our data with replacement\n",
    "\n",
    "![Bootstrapping](images/bootstrap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation - combine results of models with voting\n",
    "\n",
    "![Random Forest](images/random-forest-visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Explanation\n",
    "- Ensemble method using weak learners (usually decision trees) with high bias to iteratively improve our model by reducing loss\n",
    "- Misclassified data is more heavily weighted when run through the next weak learner (weighted bootstrapping)\n",
    "- Models run in sequence (can be bad for scalability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "- Boosting tends to reduce variance\n",
    "- Bagging tends to reduce bias, and can lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier - weights are updated for each decision tree stump based on misclassified samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![AdaBoost](images/adaboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform vs. weighted resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.arange(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [.2, .2, .2, .2, .2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.choice(dataset, size = 5, p = probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's say our first tree correctly classifies every datapoint it gets, except for datapoint 5 - probability of row 5 being drawn gets upweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [.1, .1, .1, .1, .6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(dataset, size = 5, p = probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each tree MUST be grown in sequence in order to reweight samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AdaBoost Algorithm (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ y = sign (\\sum_{t=1}^T\\alpha_t h_t(X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - y = Final classification of model (-1 or 1)\n",
    "#### - T = Our weak learner set (decision tree stump)\n",
    "#### - $\\alpha_t$ is the weight of decision tree stump t\n",
    "#### - $h_t(X)$ is the prediction (-1 or 1) of decision tree stump t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we have 10 trees in our AdaBoost algorithm. 6 vote for -1 and 4 vote for 1. The output is +2. We take just the sign and classify this row as a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Votes - stronger learners get higher proportion of votes\n",
    "\n",
    "- Remember in random forest bagging models, each tree gets an equal vote\n",
    "- This contrasts from Adaboost, where stronger learners (better classifiers) get more heavily weighted votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost vote weighting formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1 - \\text{misclassification rate}}{\\text{misclassification rate}}\\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's calculate a few possible weights to get an idea of how they impact the final classification formula "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\text{misclassification rate} = \\frac{\\text{# of misclassifications}}{\\text{# of observations}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_weight(accuracy):\n",
    "    misclass = 1 - accuracy\n",
    "    return 1/2 * np.log((1 - misclass) / misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weight(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weight(.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weight(.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weight(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weight(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample reweighting formula (not normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\text{new weight} = \\text{old weight} * e^{-\\alpha_t y_t h_t(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{To normalize - Divide by}:  $\n",
    "\n",
    "$(\\text{misclassification rate learner}) * e^{\\alpha_t} + (1 - \\text{misclassification rate}) * e^{-\\alpha_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weight_calculator(old_weight, alpha, actual_class, pred_class):\n",
    "    new_weight = old_weight * np.exp(-alpha * actual_class * pred_class)\n",
    "    return new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_calculator(old_weight = .2,\n",
    "                 alpha = 2,\n",
    "                 actual_class = 1,\n",
    "                 pred_class = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoost - Generalized boosting to minimize an arbitrary loss function over iterations\n",
    "\n",
    "### Each successive weak learner is fit based on residuals of the previous tree\n",
    "\n",
    "#### Early Iterations\n",
    "\n",
    "![GradientBoost early iterations](images/gradient-boost-viz1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Later iterations\n",
    "\n",
    "![GradientBoost later iterations](images/gradient-boost-viz2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
