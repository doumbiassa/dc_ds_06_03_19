{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Review CRISP-DM model\n",
    "- Practice running a classifcation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# CRISP-DM Model\n",
    "\n",
    "What are the major phases ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### - Business Understanding\n",
    "Why am I looking at this data set?  \n",
    "What am I trying to answer?  \n",
    "How does this data help me answer my question?  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Data Understanding and Data Preparation\n",
    "Where are the missing values?  \n",
    "What do the columns mean?  \n",
    "How do I decide on what new columns to make?  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Modeling and Evaluation\n",
    "\n",
    "How well did my model do?  \n",
    "Which metric am I using to evaluate my model?  \n",
    "What can I change to increase my scoring metric?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Deployment\n",
    "\n",
    "Who is this going to?  \n",
    "Where do I need to document better?  \n",
    "Which areas are unclear?  \n",
    "What can I do better?   \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At what stages would the following methods be used?\n",
    "- `StandardScaler()`\n",
    "- `train_test_split()`\n",
    "- `auc()`\n",
    "- `pd.dropna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This [data](https://www.kaggle.com/jolasa/bay-area-bike-sharing-trips) is taken from Kaggle. This dataset contains anonymized trips data of Lyft bike sharing system(Bay Wheels), in the Bay Area from January 2019 to May 2019.\n",
    "\n",
    "![bikes](pics/bikes.jpg) ![the wiggle](pics/wiggle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Business Understanding-\n",
    "\n",
    "We want to discover if we can predict ____ with ____\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list = glob.glob(\"data/*.csv\")\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for csv in csv_list:\n",
    "    df = pd.read_csv(csv, index_col=None, header=0)\n",
    "    list_of_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = pd.concat(list_of_dfs)\n",
    "bike_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Data Understanding and Data Preparation -\n",
    "\n",
    "We need to clean the data, look at NaN values, understand what the columns represent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Are there missing values?  \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug for `missingno`\n",
    "[missingno](https://github.com/ResidentMario/missingno) is a library to visualize \"missing data\" in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(bike_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could cause these two columns (member_birth_year and member_gender) to be missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.user_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df[pd.isnull(bike_df).any(axis=1)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do?\n",
    "\n",
    "What percentage of our data is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1053067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Are there redunant columns?  \n",
    "What ones should I keep?  \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.drop(['start_station_name', 'end_station_name'], axis = 1)\n",
    "bike_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(bike_df['member_birth_year'])\n",
    "plt.title('Distribution of Customer Ages');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.set_style('whitegrid')\n",
    "sns.distplot(bike_df['start_station_id'], label='Starting Dock', color = 'g')\n",
    "sns.distplot(bike_df['end_station_id'], label='Returning Dock', color='r')\n",
    "plt.title('Starting and Ending Location')\n",
    "plt.xlabel('Station ID')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.set_style('whitegrid')\n",
    "sns.distplot(bike_df['trip_duration_sec'])\n",
    "plt.xlabel('Second')\n",
    "plt.title('Seconds in Trip');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.trip_duration_sec.nsmallest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.trip_duration_sec.nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Lets turn the time into mins rather than seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df['trip_duration_min'] = (bike_df.trip_duration_sec / 60).round(2)\n",
    "bike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.trip_duration_min.nsmallest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.trip_duration_min.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(bike_df.trip_duration_min);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fancy function to remove outliers using IQR. Not PEP 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_names):\n",
    "    df_out = df_in\n",
    "    for col in col_names:\n",
    "        q1 = df_in[col].quantile(0.25)\n",
    "        q3 = df_in[col].quantile(0.75)\n",
    "        iqr = q3-q1 #Interquartile range\n",
    "        fence_low  = q1-1.5*iqr\n",
    "        fence_high = q3+1.5*iqr\n",
    "        df_out = df_in.loc[(df_in[col] > fence_low) & (df_in[col] < fence_high)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slimmed_df = remove_outlier(bike_df, [\"trip_duration_min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(slimmed_df.trip_duration_min);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We went from {len(bike_df)} rows to a smaller {len(slimmed_df)} by removing the IQR in the duration column.\")\n",
    "print(f\"That was a {round((len(bike_df) - len(slimmed_df)) / len(bike_df), 5)*100}% decrease in the number of rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a cleaned dataset. Depending on what we found during the EDA, or anything else that came up, we can take this in many directions. \n",
    "\n",
    "\n",
    "___\n",
    "1. First we will try to predict if the member is a \"brogrammer\".  \n",
    "2. Then we will attempt to predict the `user_type`.  \n",
    "3. Finally, we'll see if anything exists to predict `month`.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### - Modeling and Evaluation -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "Starting off with a simple model to give us a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone who is a male and lives in SF AND rides an electric bike is a Brogrammer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brogram(pd_series):\n",
    "    if not \"Male\" in pd_series:\n",
    "        return \"Nice_person\"\n",
    "    else:\n",
    "        return \"Brogrammer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = slimmed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['bg'] = df_1.apply(lambda x: brogram(x['member_gender']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.drop('member_gender', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.bg.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.get_dummies(df_1, columns=['month', 'user_type'])\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(df_1['bg'])\n",
    "data = df_1.drop(['bg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.bg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DT = DecisionTreeClassifier() #No max depth is set, be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_DT.fit(X_train, y_train)\n",
    "\n",
    "fit_time = (time.time()) - start_time\n",
    "print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_DT.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_hat) * 100\n",
    "print(f\"Accuracy Score is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(clf_DT)\n",
    "\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "\n",
    "visualizer.poof()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(clf_DT)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 5, 7, 15],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = clf_RF, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# fit_time = (time.time()) - start_time\n",
    "# print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything everyone should have seen by now, or at least encoutered **PICKLE**\n",
    "\n",
    "\n",
    "\n",
    "![pickle](pics/pickle.jpg)\n",
    "\n",
    "\n",
    "The above CV took 90 mins to run through. Imagine it took a few hours. If I wanted to store this trained model, to use or compare with later, I can _pickle_ it to use for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(grid_search, \"GridSeach_RF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = pd.read_pickle(\"GridSeach_RF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = (RandomForestClassifier(max_depth = 15,\n",
    "                                 max_features = 3,\n",
    "                                 min_samples_leaf = 3,\n",
    "                                 min_samples_split = 8,\n",
    "                                 n_estimators = 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_RF.fit(X_train, y_train)\n",
    "\n",
    "fit_time = (time.time()) - start_time\n",
    "print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_RF.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_hat) * 100\n",
    "print(f\"Accuracy Score is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(clf_RF)\n",
    "\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "\n",
    "visualizer.poof()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(clf_RF)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search vs Grid Search\n",
    "\n",
    "![grid](pics/grid.png) ![random](pics/rand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.get_dummies(slimmed_df, columns = ['month', 'member_gender'])\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(df_2['user_type'])\n",
    "data = df_2.drop('user_type', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check out how well balanced our target values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.user_type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOOF, that is not good. Not good at all. But our \"boss\" wants this done, so lets at least attempt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Santity check if everything split correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a baseline before altering the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_RF.fit(X_train, y_train)\n",
    "\n",
    "fit_time = (time.time()) - start_time\n",
    "print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_RF.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_hat) * 100\n",
    "print(f\"Accuracy Score is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(clf_RF)\n",
    "\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "\n",
    "visualizer.poof()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(clf_RF)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets just try putting in class weights and see how it improves our scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_weights = RandomForestClassifier(class_weight={'Subscriber':0.91, 'Customer':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_RF_weights.fit(X_train, y_train)\n",
    "\n",
    "fit_time = (time.time()) - start_time\n",
    "print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_RF_weights.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_hat) * 100\n",
    "print(f\"Accuracy Score is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(clf_RF_weights)\n",
    "\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "\n",
    "visualizer.poof()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(clf_RF_weights)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Model - Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_count = bike_df.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(month_count.index, month_count.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection, but also getting a percentage is wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.month.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more balanced classes. Will this help in predicting? Lets find out...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.get_dummies(slimmed_df, columns = ['user_type', 'member_gender'])\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_3['month']\n",
    "data = df_3.drop('month', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# target = le.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_multi = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_RF_multi.fit(X_train, y_train)\n",
    "\n",
    "fit_time = (time.time()) - start_time\n",
    "print(f'-------{fit_time}s seconds------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_RF_multi.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_hat) * 100\n",
    "print(f\"Accuracy Score is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(clf_RF_multi)\n",
    "\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "\n",
    "visualizer.poof()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(clf_RF_multi)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We worked hard and yet nothing seems to be great. What could we have done differently...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
