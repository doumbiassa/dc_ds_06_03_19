{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit ALS\n",
    "\n",
    "_Adapted from [@sophwatts](https://github.com/sophwats/Implicit-ALS)_\n",
    "\n",
    "We usually consider using ALS on a set of user/product ratings. But what if the data isn't so self explanatory?\n",
    "\n",
    "### A day trip to the library\n",
    "Consider, for example, the data collected by a local library. The library records which users took out each books and how long they kept the books before returning them. \n",
    "\n",
    "As such, we have no explicit indication that a user liked or disliked the books they took out - Just because you borrowed a book does not mean that you enjoyed it, or even read it.\n",
    "Furthermore, the missing data is of interest - the fact that a user has not taken out a specific book could indicate that they dislike that genre, or that they haven't been to that section of the library.\n",
    "\n",
    "Furthermore the same user action could have many different causes. Suppose you withdraw a book three times. That might indicate that you loved the book, but it may also indicate that the book doesn't appeal to you as strongly as some other books you withdrew so you never got round to reading it the first two times.\n",
    "\n",
    "To make the situation even worse, implicit data is often dirty. For example, a user may withdraw a library book for their child using their account, or they may accidentally pick up a book that was sitting on the counter. \n",
    "\n",
    "### The solution\n",
    "Based on the standard ALS implementation, [Hu et al. (2008)](http://yifanhu.net/PUB/cf.pdf) presented a methodolgy for carrying out ALS when dealing with implicit data. \n",
    "\n",
    "The general idea is that we have some recorded observations $r_{u,i}$ denoting the level of interaction user $u$ had with product $i$. For example, if a user $1$ borrowed book $4$ once we may set $r_{1,4}=1$. Alternatively we may wish to allow $r_{u,i}$ to hold information about how many days the book was borrowed for. (There is a lot of freedom in this set up, so we need to make some data specific decisions regarding how we will select $r_{u,i}$).\n",
    "\n",
    "Given the set of observations $r_{u,i}$, a binary indicator $p_{u,i}$ is introduced where:\n",
    "\n",
    "$ p_{u,i} = \\begin{cases} 1 & \\mbox{if } r_{i,j}>0 \\\\\n",
    "0 & \\mbox{otherwise.} \\end{cases} $\n",
    "\n",
    "\n",
    "A confidence parameter $\\alpha$ lets the user determine how much importance they wish to place on the recorded $r_{u,i}$. This leads to the introduction of $c_{u,i}$ which we take to be the confidence we have in the strength of user $u$'s reaction to product $i$: \n",
    "$c_{u,i} = 1 + \\alpha r_{u,i}$.\n",
    "\n",
    "Let $N_u$ denote the number of users, and $N_p$ denote the number of products. Let $k\\in \\mathbb{R}^+$ be a user defined number of factors. \n",
    "Now, in implicit ALS the goal is to find matrices $X\\in \\mathbb{R}^{N_u \\times k}$ and $Y\\in \\mathbb{R}^{N_p \\times k}$ such that the following cost function is minimised:\n",
    "\n",
    "$\\sum_{u,i} c_{u,i}(p_{u,i}-X_u^T Y_i)^2 + \\lambda (\\sum_u \\| X_u\\|^2 + \\sum_{i} \\| y_u\\|^2), $\n",
    "\n",
    "\n",
    "where\n",
    "$X_u$ is the $u$th row of X, \n",
    "$Y_i$ is the $i$th row of Y,\n",
    "\\lambda is a user defined parameter which prevents overfitting. \n",
    "\n",
    "With this minimisation at hand, we are able to recover estimates of $c_{u,i}$, and thus of $r_{u,i}$ for interactions which have not yet occured. \n",
    "\n",
    "### Let's get going\n",
    "We are going to run implicit ALS using the implementation given in the pyspark.mllib.recommendation module. \n",
    "\n",
    "The data we will be using can be found at http://www2.informatik.uni-freiburg.de/~cziegler/BX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a spark context\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder\n",
    "         .appName('implicitALS')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "\n",
    "In the cell below, we download and unzip the data. The two files we are interested in are BX-Books.csv and BX-Book-Ratings.csv, which follow these schema: \n",
    "\n",
    "### BX-Books.csv\n",
    "\n",
    "| Field Name | Type | Description |\n",
    "| ---------- | -----| ----------- |\n",
    "|ISBN |  String | length 10, alphanumeric |\n",
    "| Book-Title | String | Title of book |\n",
    "|Book-Author | String| Name of author |\n",
    "| Year-Of-Publication | String | yyyy|\n",
    "|Publisher| String |Name of publisher |\n",
    "|Image-URL-S | String| URL for small image on amazon.com |\n",
    "|Image-URL-M | String| URL for medium image on amazon.com |\n",
    "|Image-URL-L | String| URL for large image on amazon.com|\n",
    "\n",
    "\n",
    "### BX-Book-Ratings.csv\n",
    "| Field Name | Type | Description |\n",
    "| ---------- | ---- | ----------- |\n",
    "|User-ID |  Integer | Range from 2 to 278854 |\n",
    "| ISBN | String| length 10, alphanumeric |\n",
    "|Book-Rating| Integer | 1-10 denotes dislike-like. 0 denotes implicit interaction|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Downloading and unzipping the data\n",
    "!wget http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip\n",
    "!unzip BX-CSV-Dump.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above loads three .csv files into the working directory. We are interested in the files \"BX-Books-Ratings.csv\" and \"BX-Books.csv\". The first three columns of \"BX-Book-Ratings\" are the user id, an isbn which identifies the book, and a rating. A '0' in the rating column is used to denote that an implicit interaction occured between the user an the book. It is this data that we are interested in, and we extract such rows using the following grep command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head BX-Book-Ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '\"user_id\";\"isbn\";\"observation\"' > implicit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep '\"0\"' BX-Book-Ratings.csv >> implicit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head implicit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "ratings_df = spark.read.csv('implicit.csv', sep=';', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 10 entries in the ratings file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implicit ALS function we are going to use requires that product ids are integers. At the moment we have unique ISBNs, which contain a mixture of numbers and letters, so we must convert to integers. This can be done using the zipWithIndex() function which takes an RDD and joins unique ids to each entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.createGlobalTempView(\"ratings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  user_id\n",
    ", dense_rank() OVER (ORDER BY isbn) AS isbn_id\n",
    ", isbn\n",
    "-- we want 1, not 0, as having interacted with a book\n",
    ", 1 as rating\n",
    "FROM global_temp.ratings\"\"\")\n",
    "ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the ALS function from the mllib module, and build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(rank=5, maxIter=5, alpha=0.99, implicitPrefs=True,\n",
    "          userCol=\"user_id\", itemCol=\"isbn_id\", ratingCol=\"rating\",\n",
    "          nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = ratings_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the predict all function to give predictions for any unseens. \n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at predictions for a range of user, product pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.filter()` and `.orderBy()` to view the 20 highest rated items for that user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions.filter(predictions.user_id == 8)\n",
    " .orderBy(\"prediction\", ascending=False).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The .recommendForAllItems function allows us to view predicted ratings for specific user, item pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recommendForAllItems(8).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "In this notebook we saw how to build a basic implicit ALS model in Spark. However, the data used was fairly plain, with \"0\"s being used for all implicit interactions. Furtherwork should consider a dataset more suited to implicit ALS. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
